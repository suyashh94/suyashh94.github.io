- authors: ''
  date: '2026-02-12'
  has_guide: true
  id: '2406.11813'
  one_sentence_summary: This paper investigates how LLMs acquire factual knowledge
    during pretraining, revealing that knowledge acquisition occurs through micro-acquisitions
    that accumulate with a power-law forgetting pattern, and demonstrating that larger
    batch sizes and data deduplication enhance knowledge retention.
  paper_type: empirical
  title: How Do Large Language Models Acquire Factual Knowledge During Pretraining?
- authors: ''
  date: '2026-02-12'
  has_guide: true
  id: '2512.24880'
  one_sentence_summary: This paper introduces Manifold-Constrained Hyper-Connections
    (mHC), which projects residual connection matrices onto the Birkhoff polytope
    using Sinkhorn-Knopp to restore identity mapping properties and enable stable
    large-scale training of deep networks with expanded residual streams.
  paper_type: methodology
  title: 'mHC: Manifold-Constrained Hyper-Connections'
- authors: ''
  date: '2026-02-12'
  has_guide: true
  id: '2601.19897'
  one_sentence_summary: This paper introduces Self-Distillation Fine-Tuning (SDFT),
    which enables on-policy learning from expert demonstrations by using a demonstration-conditioned
    version of the same model as its own teacher, achieving new-task learning while
    substantially reducing catastrophic forgetting compared to supervised fine-tuning.
  paper_type: methodology
  title: Self-Distillation Enables Continual Learning
- authors: ''
  date: '2026-02-12'
  has_guide: true
  id: '2601.05242'
  one_sentence_summary: This paper identifies that GRPO's reward normalization collapses
    distinct multi-reward combinations into identical advantages, and proposes GDPO
    which decouples normalization per reward to preserve training signal granularity.
  paper_type: methodology
  title: 'GDPO: Group reward-Decoupled Normalization Policy Optimization'
- authors: ''
  date: '2026-02-12'
  has_guide: true
  id: '2509.04903'
  one_sentence_summary: This paper introduces ACE-RL, a reinforcement learning framework
    that enhances long-form text generation in LLMs by converting subjective quality
    evaluation into fine-grained, instruction-adaptive constraint verification.
  paper_type: methodology
  title: 'ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement
    Learning'
- authors: ''
  date: '2026-02-12'
  has_guide: true
  id: '2601.08763'
  one_sentence_summary: This paper introduces Uniqueness-Aware Reinforcement Learning,
    which improves LLM reasoning diversity by reweighting policy advantages to favor
    correct but rare solution strategies, thereby addressing exploration collapse
    in RL-trained language models.
  paper_type: methodology
  title: 'Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in
    LLMs'
- authors: ''
  date: '2026-02-12'
  has_guide: true
  id: '2509.04259'
  one_sentence_summary: When fine-tuning foundation models, RL forgets less than SFT
    because on-policy RL is implicitly biased toward KL-minimal solutions, and KL
    divergence on the new task reliably predicts the degree of catastrophic forgetting.
  paper_type: empirical+theoretical
  title: 'RL''s Razor: Why Online Reinforcement Learning Forgets Less'
